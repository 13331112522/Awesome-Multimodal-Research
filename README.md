# Awesome Multimodal Research [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

![build](https://img.shields.io/badge/build-passing-brightgreen.svg)
![license](https://img.shields.io/badge/License-MIT-brightgreen.svg)
![prs](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)

> This repo is reorganized from [Paul Liang's repo: Reading List for Topics in Multimodal Machine Learning](https://github.com/pliang279/awesome-multimodal-ml), any suggestions are welcome! 


## Research Papers

* [Survey Papers](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Survey-Papers)
* [Core Areas](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas)
  * [Representation Learning](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Representation-Learning)
  * [Multimodal Fusion](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Multimodal-Fusion)
  * [Multimodal Alignment](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Multimodal-Alignment)
  * [Multimodal Translation](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Multimodal-Translation)
  * [Missing or Imperfect Modalities](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Missing-or-Imperfect-Modalities)
  * [Knowledge Graphs and Knowledge Bases](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Knowledge-Graphs-and-Knowledge-Bases)
  * [Intepretable Learning](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Intepretable-Learning)
  * [Generative Learning](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Generative-Learning)
  * [Semi-supervised Learning](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Semi-supervised-Learning)
  * [Self-supervised Learning](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Self-supervised-Learning)
  * [Language Models](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Language-Models)
  * [Adversarial Attacks](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Adversarial-Attacks)
  * [Few-Shot Learning](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Core-Areas/Few-Shot-Learning)
* [Applications](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications)
  * [Language and Visual QA](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Language-and-Visual-QA)
  * [Language Grounding in Vision](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Language-Grounding-in-Vision)
  * [Language Grouding in Navigation](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Language-Grouding-in-Navigation)
  * [Multimodal Machine Translation](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Multimodal-Machine-Translation)
  * [Multi-agent Communication](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Multi-agent-Communication)
  * [Commonsense Reasoning](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Commonsense-Reasoning)
  * [Multimodal Reinforcement Learning](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Multimodal-Reinforcement-Learning)
  * [Multimodal Dialog](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Multimodal-Dialog)
  * [Language and Audio](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Language-and-Audio)
  * [Audio and Visual](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Audio-and-Visual)
  * [Media Description](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Media-Description)
  * [Video Generation from Text](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Video-Generation-from-Text)
  * [Affect Recognition and Multimodal Language](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Affect-Recognition-and-Multimodal-Language)
  * [Healthcare](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Healthcare)
  * [Robotics](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/papers/Applications/Robotics)
* [Workshops](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/workshops)
* [Tutorials](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/tutorials)
* [Courses](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research/tree/master/courses)


## Recent Workshop

[Visually Grounded Interaction and Language (ViGIL)](https://vigilworkshop.github.io/), NAACL 2021

[Wordplay: When Language Meets Games](https://wordplay-workshop.github.io/), NeurIPS 2020

[Grand Challenge and Workshop on Human Multimodal Language](http://multicomp.cs.cmu.edu/acl2020multimodalworkshop/), ACL 2020

[Advances in Language and Vision Research](https://alvr-workshop.github.io/), ACL 2020

[Language & Vision with applications to Video Understanding](https://languageandvision.github.io/), CVPR 2020

[International Challenge on Activity Recognition (ActivityNet)](http://activity-net.org/challenges/2020/), CVPR 2020

[The End-of-End-to-End A Video Understanding Pentathlon](https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/), CVPR 2020

[Emergent Communication: Towards Natural Language](https://sites.google.com/view/emecom2019), NeurIPS 2019

[Workshop on Multimodal Understanding and Learning for Embodied Applications](https://sites.google.com/view/mulea2019/home), ACM Multimedia 2019

[Beyond Vision and Language: Integrating Real-World Knowledge](https://www.lantern.uni-saarland.de/), EMNLP 2019

[The How2 Challenge: New Tasks for Vision & Language](https://srvk.github.io/how2-challenge/), ICML 2019






## Papers With Code

  - [Multimodal Related --- from Papers With Code](https://paperswithcode.com/search?q=multimodal)


## Research Team
  - [CMU --- MultiComp Lab](http://multicomp.cs.cmu.edu/)
  - [MIT --- SYNTHETIC INTELLIGENCE LABORATORY](http://synthintel.org/)
  - [NTU --- SenticNet Team](http://sentic.net/)
  - [SenticNet GitHub](https://github.com/SenticNet)
  - [MultiMT](https://multimt.github.io/)
  - [Microsoft --- Multimodal AI](https://multimodalai.azurewebsites.net/)
  

## Related Datasets

  - [CMU MultimodalSDK --- Affect Recognition and Multimodal Language](https://github.com/A2Zadeh/CMU-MultimodalSDK)
  - [AMHUSE --- Affect Recognition and Multimodal Language](http://amhuse.phuselab.di.unimi.it/)
  - [Multi30k Dataset --- Multimodal Machine Translation](https://github.com/multi30k/dataset)
  - [VATEX --- Multimodal Machine Translation](http://vatex.org/main/index.html)
  - [MELD --- Multimodal Dialog](https://affective-meld.github.io/)
  - [CLEVR-Dialog --- Multimodal Dialog](https://github.com/satwikkottur/clevr-dialog)
  - [Charades-Ego --- Media Description](https://allenai.org/plato/charades/)
  - [MPII --- Media Description](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/mpii-movie-description-dataset/)
  - [RecipeQA --- Language and Visual QA](https://hucvl.github.io/recipeqa/)
  - [GQA --- Language and Visual QA](https://cs.stanford.edu/people/dorarad/gqa/)
  - [CLEVR --- Language and Visual QA](https://github.com/facebookresearch/clevr-dataset-gen)
