# Awesome Multimodal Research [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

![build](https://img.shields.io/badge/build-passing-brightgreen.svg)
![license](https://img.shields.io/badge/License-MIT-brightgreen.svg)
![prs](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)

> Organize based on [Paul Liang's repo: Reading List for Topics in Multimodal Machine Learning](https://github.com/pliang279/awesome-multimodal-ml), any suggestions are welcome! 


## Research Papers

* [Survey Papers](#survey-papers)
* [Core Areas](#core-areas)
  * [Representation Learning](#representation-learning)
  * [Multimodal Fusion](#multimodal-fusion)
  * [Multimodal Alignment](#multimodal-alignment)
  * [Multimodal Translation](#multimodal-translation)
  * [Missing or Imperfect Modalities](#missing-or-imperfect-modalities)
  * [Knowledge Graphs and Knowledge Bases](#knowledge-graphs-and-knowledge-bases)
  * [Intepretable Learning](#intepretable-learning)
  * [Generative Learning](#generative-learning)
  * [Semi-supervised Learning](#semi-supervised-learning)
  * [Self-supervised Learning](#self-supervised-learning)
  * [Language Models](#language-models)
  * [Adversarial Attacks](#adversarial-attacks)
  * [Few-Shot Learning](#few-shot-learning)
* [Applications](#applications)
  * [Language and Visual QA](#language-and-visual-qa)
  * [Language Grounding in Vision](#language-grounding-in-vision)
  * [Language Grouding in Navigation](#language-grouding-in-navigation)
  * [Multimodal Machine Translation](#multimodal-machine-translation)
  * [Multi-agent Communication](#multi-agent-communication)
  * [Commonsense Reasoning](#commonsense-reasoning)
  * [Multimodal Reinforcement Learning](#multimodal-reinforcement-learning)
  * [Multimodal Dialog](#multimodal-dialog)
  * [Language and Audio](#language-and-audio)
  * [Audio and Visual](#audio-and-visual)
  * [Media Description](#media-description)
  * [Video Generation from Text](#video-generation-from-text)
  * [Affect Recognition and Multimodal Language](#affect-recognition-and-multimodal-language)
  * [Healthcare](#healthcare)
  * [Robotics](#robotics)
* [Workshops](#workshops)
* [Tutorials](#tutorials)
* [Courses](#courses)


## Papers With Code

  - [Multimodal Related --- from Papers With Code](https://paperswithcode.com/search?q=multimodal)


## Research Team
  - [CMU --- MultiComp Lab](http://multicomp.cs.cmu.edu/)
  - [MIT --- SYNTHETIC INTELLIGENCE LABORATORY](http://synthintel.org/)
  - [NTU --- SenticNet Team](http://sentic.net/)
  - [SenticNet GitHub](https://github.com/SenticNet)
  - [MultiMT](https://multimt.github.io/)
  

## Related Datasets

  - [CMU MultimodalSDK --- Affect Recognition and Multimodal Language](https://github.com/A2Zadeh/CMU-MultimodalSDK)
  - [AMHUSE --- Affect Recognition and Multimodal Language](http://amhuse.phuselab.di.unimi.it/)
  - [Multi30k Dataset --- Multimodal Machine Translation](https://github.com/multi30k/dataset)
  - [VATEX --- Multimodal Machine Translation](http://vatex.org/main/index.html)
  - [MELD --- Multimodal Dialog](https://affective-meld.github.io/)
  - [CLEVR-Dialog --- Multimodal Dialog](https://github.com/satwikkottur/clevr-dialog)
  - [Charades-Ego --- Media Description](https://allenai.org/plato/charades/)
  - [MPII --- Media Description](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/mpii-movie-description-dataset/)
  - [RecipeQA --- Language and Visual QA](https://hucvl.github.io/recipeqa/)
  - [GQA --- Language and Visual QA](https://cs.stanford.edu/people/dorarad/gqa/)
  - [CLEVR --- Language and Visual QA](https://github.com/facebookresearch/clevr-dataset-gen)
